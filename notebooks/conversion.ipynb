{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forced-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.bundled_inputs\n",
    "import torch.utils.mobile_optimizer\n",
    "import torch.backends._nnapi.prepare\n",
    "import torchvision.models.quantization.mobilenet\n",
    "from pathlib import Path\n",
    "import flash\n",
    "import flash.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "horizontal-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flash.image.ImageClassifier.backbones(name=\"mobilenet_v2_quant\")\n",
    "def fn_mobilenet_v2_quant(pretrained: bool = True):\n",
    "    model = torchvision.models.quantization.mobilenet.mobilenet_v2(pretrained=True)    \n",
    "\n",
    "    # remove the last two layers & turn it into a Sequential model\n",
    "    # backbone = torch.nn.Sequential(*list(model.children())[:-2])\n",
    "    \n",
    "    backbone = model.features\n",
    "    num_features = model.classifier[-1].in_features\n",
    "    # backbones need to return the num_features to build the head\n",
    "    return backbone, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "continued-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mobilenetv2_nnapi(output_dir_path, quantize_mode, model):\n",
    "    quantize_core, quantize_iface = {\n",
    "        \"none\": (False, False),\n",
    "        \"core\": (True, False),\n",
    "        \"full\": (True, True),\n",
    "    }[quantize_mode]\n",
    "\n",
    "    #model = torchvision.models.quantization.mobilenet.mobilenet_v2(pretrained=True, quantize=quantize_core)\n",
    "    model.eval()\n",
    "\n",
    "    # Fuse BatchNorm operators in the floating point model.\n",
    "    # (Quantized models already have this done.)\n",
    "    # Remove dropout for this inference-only use case.\n",
    "    #if not quantize_core:\n",
    "    #    model.fuse_model()\n",
    "    #assert type(model.classifier[0]) == torch.nn.Dropout\n",
    "    #model.classifier[0] = torch.nn.Identity()\n",
    "    #model.adapter.head\n",
    "    \n",
    "    \n",
    "    input_float = torch.zeros(1, 3, 224, 224)\n",
    "    input_tensor = input_float\n",
    "\n",
    "    # If we're doing a quantized model, we need to trace only the quantized core.\n",
    "    # So capture the quantizer and dequantizer, use them to prepare the input,\n",
    "    # and replace them with identity modules so we can trace without them.\n",
    "    if quantize_core:\n",
    "        quantizer = model.quant\n",
    "        dequantizer = model.dequant\n",
    "        model.quant = torch.nn.Identity()\n",
    "        model.dequant = torch.nn.Identity()\n",
    "        input_tensor = quantizer(input_float)\n",
    "\n",
    "    # Many NNAPI backends prefer NHWC tensors, so convert our input to channels_last,\n",
    "    # and set the \"nnapi_nhwc\" attribute for the converter.\n",
    "    input_tensor = input_tensor.contiguous(memory_format=torch.channels_last)\n",
    "    input_tensor.nnapi_nhwc = True\n",
    "\n",
    "    # Trace the model.  NNAPI conversion only works with TorchScript models,\n",
    "    # and traced models are more likely to convert successfully than scripted.\n",
    "    with torch.no_grad():\n",
    "        traced = torch.jit.trace(model, input_tensor)\n",
    "    nnapi_model = torch.backends._nnapi.prepare.convert_model_to_nnapi(traced, input_tensor)\n",
    "\n",
    "    # If we're not using a quantized interface, wrap a quant/dequant around the core.\n",
    "    if quantize_core and not quantize_iface:\n",
    "        nnapi_model = torch.nn.Sequential(quantizer, nnapi_model, dequantizer)\n",
    "        model.quant = quantizer\n",
    "        model.dequant = dequantizer\n",
    "        # Switch back to float input for benchmarking.\n",
    "        input_tensor = input_float.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "    # Optimize the CPU model to make CPU-vs-NNAPI benchmarks fair.\n",
    "    model = torch.utils.mobile_optimizer.optimize_for_mobile(torch.jit.script(model))\n",
    "\n",
    "    # Bundle sample inputs with the models for easier benchmarking.\n",
    "    # This step is optional.\n",
    "    class BundleWrapper(torch.nn.Module):\n",
    "        def __init__(self, mod):\n",
    "            super().__init__()\n",
    "            self.mod = mod\n",
    "        def forward(self, arg):\n",
    "            return self.mod(arg)\n",
    "    nnapi_model = torch.jit.script(BundleWrapper(nnapi_model))\n",
    "    torch.utils.bundled_inputs.augment_model_with_bundled_inputs(\n",
    "        model, [(torch.utils.bundled_inputs.bundle_large_tensor(input_tensor),)])\n",
    "    torch.utils.bundled_inputs.augment_model_with_bundled_inputs(\n",
    "        nnapi_model, [(torch.utils.bundled_inputs.bundle_large_tensor(input_tensor),)])\n",
    "\n",
    "    # Save both models.\n",
    "    model._save_for_lite_interpreter(str(output_dir_path / (\"mobilenetv2-quant_{}-cpu.pt\".format(quantize_mode))))\n",
    "    nnapi_model._save_for_lite_interpreter(str(output_dir_path / (\"mobilenetv2-quant_{}-nnapi.pt\".format(quantize_mode))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "thirty-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/tmp/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "indirect-perth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts/model/model_mobilenet_v2_quant.pt\n"
     ]
    }
   ],
   "source": [
    "!ls artifacts/model/model_mobilenet_v2_quant.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "romantic-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_model_path=\"artifacts/model/model_mobilenet_v2_quant.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adjacent-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = flash.image.ImageClassifier.load_from_checkpoint(artifact_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "    # Fuse BatchNorm operators in the floating point model.\n",
    "    # (Quantized models already have this done.)\n",
    "    # Remove dropout for this inference-only use case.\n",
    "    #if not quantize_core:\n",
    "    #    model.fuse_model()\n",
    "    #assert type(model.classifier[0]) == torch.nn.Dropout\n",
    "    #model.classifier[0] = torch.nn.Identity()\n",
    "    #model.adapter.head\n",
    "    \n",
    "    \n",
    "input_float = torch.zeros(1, 3, 224, 224)\n",
    "input_tensor = input_float\n",
    "\n",
    "# If we're doing a quantized model, we need to trace only the quantized core.\n",
    "# So capture the quantizer and dequantizer, use them to prepare the input,\n",
    "# and replace them with identity modules so we can trace without them.\n",
    "if quantize_core:\n",
    "    quantizer = model.quant\n",
    "    dequantizer = model.dequant\n",
    "    model.quant = torch.nn.Identity()\n",
    "    model.dequant = torch.nn.Identity()\n",
    "    input_tensor = quantizer(input_float)\n",
    "\n",
    "# Many NNAPI backends prefer NHWC tensors, so convert our input to channels_last,\n",
    "# and set the \"nnapi_nhwc\" attribute for the converter.\n",
    "input_tensor = input_tensor.contiguous(memory_format=torch.channels_last)\n",
    "input_tensor.nnapi_nhwc = True\n",
    "\n",
    "# Trace the model.  NNAPI conversion only works with TorchScript models,\n",
    "# and traced models are more likely to convert successfully than scripted.\n",
    "with torch.no_grad():\n",
    "    traced = torch.jit.trace(model, input_tensor)\n",
    "nnapi_model = torch.backends._nnapi.prepare.convert_model_to_nnapi(traced, input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-killer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-negotiation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "rocky-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pt/lib/python3.8/site-packages/torch/_jit_internal.py:668: LightningDeprecationWarning: The `LightningModule.loaded_optimizer_states_dict` property is deprecated in v1.4 and will be removed in v1.6.\n",
      "  if hasattr(mod, name):\n",
      "/opt/conda/envs/pt/lib/python3.8/site-packages/torch/_jit_internal.py:668: LightningDeprecationWarning: The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7. Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.\n",
      "  if hasattr(mod, name):\n",
      "/opt/conda/envs/pt/lib/python3.8/site-packages/torch/_jit_internal.py:669: LightningDeprecationWarning: The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7. Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.\n",
      "  item = getattr(mod, name)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-845d6904991f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#model = torchvision.models.mobilenet.mobilenet_v2(pretrained=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmake_mobilenetv2_nnapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"mobilenetv2-nnapi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantize_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-9fdfe8dea5b0>\u001b[0m in \u001b[0;36mmake_mobilenetv2_nnapi\u001b[0;34m(output_dir_path, quantize_mode, model)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtraced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mnnapi_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nnapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_model_to_nnapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# If we're not using a quantized interface, wrap a quant/dequant around the core.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pt/lib/python3.8/site-packages/torch/backends/_nnapi/prepare.py\u001b[0m in \u001b[0;36mconvert_model_to_nnapi\u001b[0;34m(model, inputs, serializer)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_model_to_nnapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     (shape_compute_module, ser_model_tensor, used_weights, inp_mem_fmts, out_mem_fmts,\n\u001b[0;32m---> 81\u001b[0;31m      retval_count) = process_for_nnapi(model, inputs, serializer)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     nnapi_model = NnapiModule(\n",
      "\u001b[0;32m/opt/conda/envs/pt/lib/python3.8/site-packages/torch/backends/_nnapi/prepare.py\u001b[0m in \u001b[0;36mprocess_for_nnapi\u001b[0;34m(model, inputs, serializer)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserializer\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_NnapiSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     (ser_model, used_weights, inp_mem_fmts, out_mem_fmts, shape_compute_lines,\n\u001b[0;32m--> 125\u001b[0;31m      retval_count) = serializer.serialize_model(model, inputs)\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mser_model_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mser_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pt/lib/python3.8/site-packages/torch/backends/_nnapi/serializer.py\u001b[0m in \u001b[0;36mserialize_model\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0mLOG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing node #%d: %r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mretn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pt/lib/python3.8/site-packages/torch/backends/_nnapi/serializer.py\u001b[0m in \u001b[0;36madd_node\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsupported node kind (%r) in node %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         \u001b[0madder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_identity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pt/lib/python3.8/site-packages/torch/backends/_nnapi/serializer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_cat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;34m\"aten::mean\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;34m\"aten::quantize_per_tensor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pt/lib/python3.8/site-packages/torch/backends/_nnapi/serializer.py\u001b[0m in \u001b[0;36madd_mean\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0min_oper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_order\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDimOrder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHANNELS_LAST\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mcollapsed_dims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m             \u001b[0mout_dim_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDimOrder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRESUMED_CONTIGUOUS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for quantize_mode in [\"none\", \"core\", \"full\"]:\n",
    "    #model = torchvision.models.quantization.mobilenet.mobilenet_v2(pretrained=True, quantize=quantize_core)    \n",
    "    \n",
    "    #model = torchvision.models.mobilenet.mobilenet_v2(pretrained=True)    \n",
    "    make_mobilenetv2_nnapi(base_path / \"mobilenetv2-nnapi\", quantize_mode, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "flexible-estimate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (train_metrics): ModuleDict(\n",
       "    (accuracy): Accuracy()\n",
       "  )\n",
       "  (val_metrics): ModuleDict(\n",
       "    (accuracy): Accuracy()\n",
       "  )\n",
       "  (test_metrics): ModuleDict(\n",
       "    (accuracy): Accuracy()\n",
       "  )\n",
       "  (adapter): DefaultAdapter(\n",
       "    (backbone): Sequential(\n",
       "      (0): ConvNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (7): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (8): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (9): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (10): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (11): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (12): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (13): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (14): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (15): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (16): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (17): QuantizableInvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (18): ConvNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=1280, out_features=101, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "driven-saturday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1280, out_features=101, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.adapter.head\n",
    "model_quant.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fiscal-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torchvision.models.mobilenet.mobilenet_v2(pretrained=True)    \n",
    "model_quant = torchvision.models.quantization.mobilenet.mobilenet_v2(pretrained=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "active-carpet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=False)\n",
       "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "??model_quant.fuse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant.fuse_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-index",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.mobilenet.mobilenet_v2(pretrained=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/model/mobilenetv2-nnapi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/model/mobilenetv2-nnapi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls  /tmp/model/mobilenetv2-nnapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.quantization.mobilenet.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "?torchvision.models.quantization.mobilenet.mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-spray",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "rapids-gpu.0-18.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/rapids-gpu.0-18:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pt]",
   "language": "python",
   "name": "conda-env-pt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
